---
title: 'Blue Lab Association Testing Pipeline Vignette'
author: 'Jai Broome'
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    keep_md: true
    toc: true
    toc_depth: 3
    number_sections: yes
    code_folding: show
---

# Introduction

This pipeline provides a friendly user interface for the conversion,
manipulation and analysis of genetic data. It expects genotype data to be in
[GDS format](https://github.com/zhengxwen/gdsfmt) and uses the [`SeqArray`
interface](https://github.com/zhengxwen/SeqArray). This pipeline is also
compatible with dosage genotype data (like you would get from the output of the
[TOPMed Imputation Server](https://imputation.biodatacatalyst.nhlbi.nih.gov/)).
Some of the scripts take the `--dosage` flag. See the script help pages for
details.

Additionally, users will need phenotype data with an outcome variable, and
age-at-event variable if doing survival analysis, saved as an R object (.rds
file). If applicable, users will also need covariate variables in the phenotype
file and a pedigree file to compare to a genetic relatedness matrix.

Lastly, tasks like resolving sample swaps, pedigree errors and phenotype data
QC are beyond the scope of this vignette, but see [INSERT RESOURCES]. The example data
used here have already been through these QC steps.

# Setup

## Save start time

This is used to calculate the amount of time needed to render this document.

```{r st}
st <- Sys.time()
```

## R Libraries

This reproducible Rmarkdown document makes use of the following packages:

```{r library, message = FALSE}
library(magrittr)
library(ggplot2)
library(knitr)
library(dplyr)
```

Additionally, when we use functions in other packages, we use the explicit
`namespace::function()` syntax to clearly show when use other packages.

## Knitr chunk options

The Blue Lab Association Pipeline is set up to be invoked from the \*NIX shell,
bash in this example. Rmarkdown/knitr allow for executable chunks in different
shells and programming languages, see the
[knitr documentation](https://bookdown.org/yihui/rmarkdown/language-engines.html)
for details.

Specify the `-l` option for bash, see `man bash` for details. Executable chunks
that use an engine _besides R_ will have a "**bold**" tag preceeding them.

```{r bash_options}
opts_chunk$set(engine.opts = list(bash = "-l"))
```

Specify that executed code and its output should be in the same chunk. This
allows you to hide long output by clicking the "Hide" button.

```{r collapse}
opts_chunk$set(collapse = TRUE)
```

## Make subdirectories

**bash:**

```{bash}
mkdir tmp vignette_fig vignette_out
# Clean up files from previous times this document may have been rendered
# touch temporary files so script doesn't crash if directories are empty
touch tmp/deleteme vignette_fig/deleteme vignette_out/deleteme
rm tmp/* vignette_fig/* vignette_out/*
```

## Note: piping with `magrittr`

This document relies heavily on the pipe operators in the `magrittr` package,
chiefly `%>%` and `%<>%`. Briefly, `foo <- x %>% bar() %>% baz()` is equivalent to
`foo <- baz(bar(x))` and `foo %<>% bar()` is equivalent to `foo <- bar(foo)`.

See `vignette("magrittr")` for more.

# GDS conversion

The first step will be to convert our data to
GDS.  This vignette will use a subset of PLINK binary data that has been
subsetted to a random 100,000 variants and approximately 1,700 samples.

This step took approximately 15 minutes on an example dataset (not the
subsetted dataset used in this dataset to speed up computation time).

Pass the path and prefix of the bed/bim/fam files to `plink_to_gds.R`.

## `plink_to_gds.R`

**bash:**

```{bash}
plink_to_gds.R \
  /nfs/beluga0_home/ANALYSIS/VIGNETTE/vignette_data/out/vignette \
  --out_file vignette_out/example.gds
```

# Preliminary QC

Now that we have have the genetic data in GDS format, we can move onto the
next steps. Typically, we'll begin by calculating sample- and
variant-level metrics.

## `sample_qc.R`

In the `tmp/` directory, this script will create `missing_by_sample.rds` and
`missing_by_sample.png`. We'll move the figures to the `vignette_fig/` directory so
`tmp/` can be removed at the end.

**bash:**

```{bash}
sample_qc.R vignette_out/example.gds --out_prefix tmp/
mv tmp/missing_by_sample.png vignette_fig/
```

There is a small number of samples with missingness > 0.3, and the rest have
low missingness.

```{r sample_missing_fig}
include_graphics("vignette_fig/missing_by_sample.png")
```

## `variant_qc.R`

In the `tmp/` directory, this script will create `variant_metrics.rds` which
includes variant missingness and MAF, `missing_by_variant.png` and `maf.png`.

**bash:**

```{bash}
variant_qc.R \
  vignette_out/example.gds \
  --out_prefix tmp/ \
  --ms_xmax 0.12
mv tmp/*.png vignette_fig
```

```{r var_missing_fig}
include_graphics("vignette_fig/missing_by_variant.png")
```

```{r maf_fig}
include_graphics("vignette_fig/maf.png")
```

It's common to practice to filter variants and samples on missingness and MAF.
Many scripts in the pipeline will take variant and sample indices for
filtering the GDS before running.

```{r samp_qc}
samp_qc <- readRDS("tmp/missing_by_sample.rds")
summary(samp_qc$missing.rate)
```

Filter out the the samples with high missingness and save the sample IDs to
disk.

```{r samp-ms-keep}
samp_pass_qc <- filter(samp_qc, missing.rate < 0.05)
saveRDS(samp_pass_qc$sample.id, "tmp/samp_pass_qc.rds")
```

There are variants with high missingness...

```{r varmet}
varmet <- readRDS("tmp/variant_metrics.rds")
summary(varmet$missing.rate)
```

...as well as rare variants...

```{r maf}
summary(varmet$maf)
```

...so we'll create a variant keep list, and save it in the `tmp/` directory:

```{r init_qc_keep}
filter(varmet, missing.rate < 0.05, maf > 0.05 )$variant.id %>%
  saveRDS("tmp/vars_qc_keep.rds")
```

# Relatedness and ancestry

This is one of the most computationally intensive steps. It took
approximately 97 minutes to run on an example dataset.

## `ld_pruning.R`

It is common practice to LD prune variants before calculating PCs and GRMs.
Here, we use the default LD threshold of $\sqrt{0.1}$. `ld_pruning.R` also can
take keep lists with the `--variant_id` and `--sample_id` options. This script
outputs a vector of LD-pruned variant IDs `vignette_out/pruned_snps.rds`.

**bash:**

```{bash}
ld_pruning.R \
  vignette_out/example.gds \
  --out_prefix vignette_out/ \
  --sample_id  tmp/samp_pass_qc.rds \
  --variant_id tmp/vars_qc_keep.rds
```

## `king_grm.R`

The rest of steps will need a GRM. We use KING to generate initial estimates
for relatedness. We supply a vector of sample- and variant-QC'd and LD pruned
variants. This script outputs two RDS files in the `tmp/` directory:

1. `king_out.rds`, the whole output from `snpgdsIBDKING()`, and
1. `king_grm.rds`, just the GRM from the `snpgdsIBDKING()` output.

KING calculates relatedness within and between families differently, so we
create a vector of FIDs and save as a .rds object to pass to KING. See
`?SNPRelate::snpgdsIBDKING` for details. Use the `--num_core` argument to
parallelize across cores to speed up computation time.

```{r king_fids}
fam <- "/nfs/beluga0_home/ANALYSIS/VIGNETTE/vignette_data/out/vignette.fam" %>%
  read.table(sep = " ", header = FALSE,
             col.names = c("FID", "IID", "FATHER", "MOTHER", "SEX", "PHENOTYPE")) %>%
  filter(IID %in% samp_pass_qc$sample.id)
saveRDS(fam$FID, "tmp/fid_keep.rds")
# Overwrite to make sure it's in the right order
saveRDS(fam$IID, "tmp/samp_pass_qc.rds")
```

**bash:**

```{bash}
king_grm.R \
  vignette_out/example.gds \
  --out_prefix vignette_out/ \
  --variant_id vignette_out/pruned_snps.rds \
  --sample_id  tmp/samp_pass_qc.rds \
  --family_id tmp/fid_keep.rds \
  --num_core 12
```

## `kinship_plot.R`

With the GRM we can plot kinship and IBS0 values, which can help us quickly
identify duplicate samples or cryptic relatedness in the sampleset.

**bash:**

```{bash}
kinship_plot.R vignette_out/king_out.rds --is_king --out_file vignette_fig/kinship.png
```

Note the points clustered at (0.0, 0.5). These represent monozygotic twins in
our dataset, but are often indicative of duplicate samples.

```{r kinship_fig}
include_graphics("vignette_fig/kinship.png")
```

## `pcair.R`

We won't use the KING GRM for association testing, but it does allows us to
select an unrelated sample set for PCA. Here we use the default kinship and
divergence thresholds of $2^{-9/2}$ and $-2^{-9/2}$, respectively. `pcair.R`
also uses the optional `--num_core` argument.

This script saves several outputs from `pcair()`:

1. `pcair.rds`, the whole output
1. `pcair_pcs.rds`, just the principle components
1. `pcair_rels.rds`, a vector of sample IDs of related individuals
1. `pcair_unrels.rds`, a vector of sample IDs of _unrelated_ individuals

These are saved to disk in the `tmp/` directory, with the prefix `a_` for the
first iteration e.g. `tmp/a_pcair.rds`.

**bash:**

```{bash}
pcair.R \
  vignette_out/example.gds \
  vignette_out/king_grm.rds \
  vignette_out/king_grm.rds \
  --out_prefix vignette_out/a_ \
  --variant_id vignette_out/pruned_snps.rds \
  --sample_id  tmp/samp_pass_qc.rds \
  --num_core 12
```

## `pca_plots.R`

PC-Relate requires us to specify the number of principal components to use.
Two common ways to determine how many is to look at the pair-wise scatterplots
and see which PCs seem to be separating known populations groups; and looking
at the percent of the variance explained by each PC.

To compare ancestry groups that appear in PC space to self-reported race or ancestry,
`pca_plots.R` can take a phenotype annotated dataframe in .rds format. We create
one here from the data downloaded from NIAGADS. We'll save the preparation of
the final phenotype file for when we have PCs from the second iteration of PC-AiR.

```{r pca-adf}
data_dir <- "/nfs/beluga0_home/DATA/NIAGADS/NG00020"
pheno_dir <- "NG00020_LOAD_phenotype_files/NG00020_LOAD_phenotype/phenotype/"
phen <- file.path(data_dir, pheno_dir, "2009_08_24_phenotype_data.csv") %>%
  read.csv(as.is = TRUE) %>%
  mutate_at(vars(SUBJ_NO, Race), as.character)
# pca_plots.R expects a variable named sample.id, so rename before writing to disk
phen_adf <- rename(phen, sample.id = SUBJ_NO) %>%
  Biobase::AnnotatedDataFrame()
saveRDS(phen_adf, "vignette_out/convert_adf.rds")
```

The phenotype annotated dataframe is now saved to disk and we pass it to
`pca_plots.R`.

**bash:**

```{bash}
pca_plots.R \
  vignette_out/a_pcair.rds \
  --out_prefix vignette_fig/a_ \
  --group Race \
  --n_pairs 6 \
  --phenotype_file vignette_out/convert_adf.rds
```

```{r pca_a_figs}
include_graphics(c("vignette_fig/a_pairs.png", "vignette_fig/a_pc_scree.png"))
```

The pairwise plots turn into clouds after about the 5th PC, so we'll use 5 for
the next step.

## `pcrelate.R`

Now that we have PCs, we can use PC-Relate, our preferred method for creating
the GRM.  PC-Relate is our preferred method because it adjusts for ancestry
PCs, so the resulting GRM only represents recent relatedness. The size of the
GRM grows greatly as the number of samples increases. To reduce the GRM size
and the computational requirements, consider using the optional
`--sparse_thresh` argument.  A good starting threshold is fourth-degree
relationships i.e. $2^{-11/2} \approx 0.022097$

This script saves the following in the `tmp/` directory:

1. `a_pcrelate.rds`, the full output from `pcrelate()`
1. `a_pcr_mat.rds`, just the GRM

**bash:**

```{bash}
pcrelate.R \
  vignette_out/example.gds \
  vignette_out/a_pcair.rds \
  --out_prefix tmp/a_ \
  --n_pcs 5 \
  --variant_id vignette_out/pruned_snps.rds \
  --sample_id  tmp/samp_pass_qc.rds \
  --sparse_thresh 0.022097
```

## `pcair.R` & `pcrelate.R`: second iteration

Our standard approach is to do two iterations of PC-AiR and PC-Relate. Note two
important differences in the second iteration:

  1. `pcair.R` now takes the PC-Relate matrix as the `kin_file`, but _still
     uses the KING GRM for the_ `div_file`.
  1. `pcrelate.R` needs the `--scale_kin 2` option.

We'll use the prefix `b_` for the second iteration output.

**bash:**

```{bash}
pcair.R \
  vignette_out/example.gds \
  tmp/a_pcr_mat.rds \
  vignette_out/king_grm.rds \
  --out_prefix tmp/b_ \
  --variant_id vignette_out/pruned_snps.rds \
  --sample_id  tmp/samp_pass_qc.rds \
  --num_core 12

pca_plots.R \
  tmp/b_pcair.rds \
  --out_prefix vignette_fig/b_ \
  --group Race \
  --n_pairs 6 \
  --phenotype_file vignette_out/convert_adf.rds
```

The pairwise PC plots look similar. Continue with 5 PCs.

```{r pca_b_figs}
include_graphics("vignette_fig/b_pairs.png")
```

```{bash}
pcrelate.R \
  vignette_out/example.gds \
  tmp/b_pcair.rds \
  --out_prefix tmp/b_ \
  --n_pcs 5 \
  --variant_id vignette_out/pruned_snps.rds \
  --sample_id  tmp/samp_pass_qc.rds \
  --scale_kin 2 \
  --sparse_thresh 0.022097
```

If you want, you can remove the data files from the first iteration with the
"a\_" prefix. Make sure to not delete any figures you might want to keep.

**bash:**

```{bash}
rm tmp/a_*
```

# Phenotype preparation

This will be one of the hardest steps to automate and generally requires
hands-on manipulation of the data.  For that reason, here we start
with a phenotype file that's already been cleaned and formatted, converted to
an annotated dataframe, and saved as an RDS file.

Make sure that the sample ID field in the phenotype file is of the same type
as the GDS sample ID, and named "sample.id". Make sure that the sex variable
is encoded as either "M"/"F" or `1L`/`2L`.

To set things up for this vignette, we read in the phenotype file and select
the variables of interest.

```{r phen-sub}
phen.sub <- "/nfs/beluga0_home/ANALYSIS/NIAGADS/NG00020/pheno/out/pheno_adf.rds" %>%
  readRDS() %>%
  Biobase::pData() %>%
  select(sample.id, Case_Control, SEX, Race, Hispanic, age, E2, E4) %>%
  mutate(across(Case_Control, as.integer))
```

## Join PCs

We join the PCs with the phenotype data before making sure the ID order
in the phenotype object is identical to the GDS ids (make sure they're both of
the same type, integer in this example).

```{r join-pcs}
pcs <- readRDS("tmp/b_pcair_pcs.rds") %>%
  as.data.frame() %>%
  set_colnames(paste0("pc", 1:32))
pcs$sample.id <- as.integer(rownames(pcs))
rownames(pcs) <- NULL

gds <- SeqArray::seqOpen("vignette_out/example.gds")
gds.id <- SeqArray::seqGetData(gds, "sample.id")
pheno <- left_join(phen.sub, pcs, "sample.id") %>%
  filter(sample.id %in% gds.id)
pheno <- pheno[match(gds.id, pheno$sample.id), ]
```

## Run Cox PH regression

Now that the PCs are joined into the phenotype dataframe, we can run the Cox PH
regression and calculate the residuals, which will be the phenotypes we'll
pass to the null model. To do this, simply pass the phenotype dataframe and the
names of the age, case status, and covariate variables to `make_resids()`.

```{r make_resids}
covars <- c("SEX", "E2", "E4", "pc1", "pc2", "pc3", "pc4", "pc5")
pheno %<>% blPipeline::make_resids("age", "Case_Control", covars = covars)
```

## Preparing the data dictionary

We will turn this into an AnnotatedDataFrame using the metadata from
the data dictionary. First, though, we've added some new variables that need the
metadata defined

```{r dd}
dd <- file.path(data_dir, pheno_dir, "2009_07_24_data_dictionary.csv") %>%
  read.csv(as.is = TRUE)
dd.sub <- filter(dd, VARNAME %in% names(phen.sub)) %>%
    select(VARNAME, VARDESC, VALUES)
```

```{r reorder-dd}
names(pheno)[!(names(pheno) %in% dd.sub$VARNAME)]
```

```{r dd.other}
dd.other <- rbind(
  data.frame(VARNAME = "sample.id", VARDESC = "Sample ID", VALUES = NA),
  c("age", "Subject's age", NA),
  c("E2", "APOE E2 allele count", NA),
  c("E4", "APOE E4 allele count", NA),
  c("surv", "Surv object for use in the survival package", NA),
  c("martingale", "Martingale residuals from a Cox PH regression", NA),
  c("devres", "Devience residuals from a Cox PH regression", NA)
)

dd.pcs <- data.frame(
  VARNAME = paste0("pc", 1:32),
  VARDESC = sprintf("Principle component %s from PC-AiR (second iteration)", 1:32),
  VALUES = NA
)

# Reorder to match phenotype variable order.
dd.all <- rbind(dd.sub, dd.other, dd.pcs)
idx <- match(names(pheno), dd.all$VARNAME)
dd.all <- dd.all[idx, ]
```

```{r adf.out}
pheno_adf <- Biobase::AnnotatedDataFrame(pheno)
meta <- setNames(dd.sub$VARDESC, dd.sub$VARNAME)
Biobase::varMetadata(pheno_adf)[names(meta), "labelDescription"] <- meta

## rename to match expected values for SeqVarTools
Biobase::varLabels(pheno_adf)[Biobase::varLabels(pheno_adf) == "SEX"] <- "sex"

## check by constructing a SeqVarData object
seqData <- SeqVarTools::SeqVarData(gds, pheno_adf)

saveRDS(pheno_adf, "vignette_out/pheno_adf.rds")
```

# Additional QC

## HWE

Deviations from Hardy-Weinberg equilbrium (HWE) can be due to genotyping
errors, but can also be caused by family or population structure, or even by
case-control differences in the exact variants we are trying to detect in GWAS.
It is importont to control for these factors when testing HWE (see
[Laurie et al 2010](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3061487/)).

### `run_ruth.sh`

Our preferred method is RUTH, which can calculate HWE on a set of diverse
samples by adjusting for principle components. Users can do this manually
by running RUTH, but can also use the wrapper script `run_ruth.sh` to:

1. Get a vector of control sample IDs from a phenotype file
1. Run PC-AiR to calculate a set of unrelateds _from the controls_
1. Concatenate FID and sample ID
1. Create a VCF subset of just the unrelated controls from PLINK .bed/.bim/.fam
   files. This step is currently recquired because to option to restrict RUTH
   to a subset of samples is not yet implemented.
1. Format PC-AiR PCs into a RUTH-compatible format
1. Run RUTH
1. Parse the RUTH output (VCF) into a tabular R object

`run_ruth.sh` is a bash script that uses getopts to parse command-line options.
There is not an auto-generated help command like there is for the R scripts in
this pipeline, but see the `while getopts` section at the top of the script
for a description of the options. For this example, I have also included a
description for each option explaining what they are.

In this example, the arguments are:

```
-p vignette_out/pheno_adf.rds: Phenotype annotated dataframe in .rds format
-d Case_Control: Case/control or dx variable in phenotype file
-o tmp/: Prefix for output files
-g vignette_out/example.gds: Genotype file in .gds format
-D tmp/b_pcr_mat.rds: GRM to be used as div object
-k vignette_out/king_grm.rds: GRM to be used as kin object
-v vignette_out/pruned_snps.rds: A vector of variant IDs to keep, in .rds format
-n 5: Number of PCs to use when running RUTH
-N 12: Number of cores to use
-P /nfs/beluga0_home/ANALYSIS/VIGNETTE/vignette_data/out/vignette: PLINK .bed/.bim/.fam files
```

**bash:**

```{bash}
run_ruth.sh \
  -p vignette_out/pheno_adf.rds \
  -d Case_Control \
  -o tmp/ \
  -g vignette_out/example.gds \
  -D tmp/b_pcr_mat.rds \
  -k vignette_out/king_grm.rds \
  -v vignette_out/pruned_snps.rds \
  -n 5 \
  -N 12 \
  -P /nfs/beluga0_home/ANALYSIS/VIGNETTE/vignette_data/out/vignette
```

We'll apply a relatively strict threshold of p<0.01 to exclude variants outside
of HWE. Note that `ABS_PVAL` from the RUTH output is the absolute value of the
negative log pvalue.  See [Kwong et al
(2021)](https://academic.oup.com/genetics/article/218/1/iyab044/6171183) for
details.

```{r ruth-exclude}
id_map <- data.frame(
  variant.id = SeqArray::seqGetData(gds, "variant.id"),
  annot.id = SeqArray::seqGetData(gds, "annotation/id")
)
ruth <- readRDS("tmp/ruth.rds") %>%
  left_join(id_map, c(ID = "annot.id")) %>%
  filter(ABS_PVAL < 2)
```

And ahead of association testing, let's subset our variants to autosomal
variants.

```{r}
ruth_auto <- filter(ruth, between(CHROM, 1, 22))
vars_keep <- readRDS("tmp/vars_qc_keep.rds")
vars_ruth_rmd <- vars_keep[vars_keep %in% ruth_auto$variant.id]
saveRDS(vars_ruth_rmd, "tmp/vars_ruth_rmd.rds")
```

# Null model and association testing

Running the actual analysis is broken into two steps: fitting the null model,
and running association testing. This is also a computationally intensive step.
It took approximately 94 minutes to run on an example dataset.

## Create sample and/or variant keep list

First, we'll need to create keep lists from our previous QC steps. We can use
`tmp/vars_ruth_rmd.rds` that we just prepared.

## `fit_null_mod.R`

To fit the null model, we need to specify the outcome variable, any covariates,
and specify the distribution family. The null model object gets saved to disk
so we can pass it to the association testing script, as well as dig into the
details if necessary.

For our default AAO analysis, the `--outcome` we pass is the Martingale residuals
from the Cox regression, and the `--covars` are the same variables we used as
covariates in the Cox regression (sex, E2, E4, and pc1-pc5 in this example).

**bash:**

```{bash}
fit_null_mod.R \
  vignette_out/pheno_adf.rds \
  --grm_file tmp/b_pcr_mat.rds \
  --sample_id tmp/samp_pass_qc.rds \
  --outcome martingale \
  --family gaussian \
  --out_file tmp/nullmod.rds \
  --covars sex E2 E4 pc1 pc2 pc3 pc4 pc5
```

## `association_test.R`

We now have everything we need to run the association testing. This script
saves the output of `assocTestSingle()` to `tmp/assoc.rds`.

This vignette uses a small subset of a real dataset, so the association testing
runs quickly. However, with larger datasets, this can be a computationally
intensive step. `association_test.R` takes the optional `--chromosome` argument
to provide an easy interface for parallelizing, and `assoc_combine.R` can
combine the outputs. Use the `--help` flag for details.

**bash:**

```{bash}
association_test.R \
  vignette_out/example.gds \
  vignette_out/pheno_adf.rds \
  --out_prefix tmp/ \
  --sample_id tmp/samp_pass_qc.rds \
  --variant_id tmp/vars_qc_keep.rds \
  --null_model tmp/nullmod.rds
```

## `assoc_plots.R`

Typically, the first things an analyst will want to check after running the
association testing are the Manhattan and QQ plot:

**bash:**

```{bash}
assoc_plots.R tmp/assoc.rds --out_prefix vignette_fig/
```

```{r qq}
include_graphics("vignette_fig/qq.png")
```

The above plot includes the calculated lambda-50, but we can also calculate it
from the p-values in the association test results.

```{r lambda}
assoc <- readRDS("tmp/assoc.rds")
blPipeline::calculate_lambda((assoc$Score.Stat)^2, df = 1)
```

This toy example is under-powered to detect any significatn associations:

```{r manh}
include_graphics("vignette_fig/manh.png")
```

# Wrap-up

Time to render document:

```{r doc-time}
Sys.time() - st
```

Clean up data files created to render this document:

*bash:*

```{bash}
#rm -r tmp
```

Close the connection to the GDS file.

```{r seqClose}
SeqArray::seqClose(gds)
```

# Addenda

## Troubleshooting and common errors

* Many steps require standardized ID field type. For example, if `sample.id` in
  a GDS is of type "integer" but type "character" in a phenotype file, certain
  steps will fail.
* Many functions require the sample ID field to be named "sample.id".
* Fitting the null model requires the sample ID fields in the GDS and phenotype
  to be identical (same samples in the same order). KING requires the sample ID
  and FID vectors to be sorted in the same order.

## Run PC-Relate by block

PC-Relate is one of the most computationally demanding steps, but it can be run
by block either in parallel or sequentially. See [here](./pcrelate_by_block.md)
for an example.
